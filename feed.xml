<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://tianlab-bioinfo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tianlab-bioinfo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-15T08:48:46+00:00</updated><id>https://tianlab-bioinfo.github.io/feed.xml</id><title type="html">blank</title><subtitle>Tian Lab is a bioinformatics lab at Fudan University. </subtitle><entry><title type="html"></title><link href="https://tianlab-bioinfo.github.io/blog/2025/2025-06-06-phenodp/" rel="alternate" type="text/html" title=""/><published>2025-06-15T08:48:46+00:00</published><updated>2025-06-15T08:48:46+00:00</updated><id>https://tianlab-bioinfo.github.io/blog/2025/2025-06-06-phenodp</id><content type="html" xml:base="https://tianlab-bioinfo.github.io/blog/2025/2025-06-06-phenodp/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/phenodp/1-480.webp 480w,/assets/img/blog/phenodp/1-800.webp 800w,/assets/img/blog/phenodp/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/phenodp/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>2025年6月6日，复旦大学生命科学学院田卫东教授课题组在 Genome Medicine期刊在线发表了题为《PhenoDP: Leveraging Deep Learning for Phenotype-Based Case Reporting, Disease Ranking, and Symptom Recommendation》的研究论文。该研究开发了一种新颖的表型驱动诊断工具 PhenoDP，融合大型语言模型（LLM）与对比学习技术，显著提升了孟德尔遗传病的诊断效率与准确率，为临床罕见病辅助诊断提供了高效、智能的解决方案。</p> <p>研究背景</p> <p>单基因遗传病（孟德尔疾病）影响全球约8%人口，早期精准诊断对改善患者预后具有重要意义。然而，由于表型信息的</p> <p>完整性与疾病表征的复杂性，现有表型驱动诊断工具在疾病排序与症状推荐上仍面临挑战。PhenoDP正是为解决这些瓶颈而提出的新一代诊断辅助工具。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/phenodp/2-480.webp 480w,/assets/img/blog/phenodp/2-800.webp 800w,/assets/img/blog/phenodp/2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/phenodp/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="图1 PhenoDP框架组成与算法流程" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>PhenoDP包含三大核心模块（图1）：</p> <ol> <li>Summarizer：基于蒸馏训练的Bio-Medical-3B-CoT模型，输入HPO术语，输出高质量、以患者为中心的临床摘要，提升症状的可解释性。</li> </ol> <p>2.Ranker：融合IC值、Phi系数和图卷积网络（GCN）三种相似性计算方法，实现精准疾病排序，特别在复杂疾病中表现突出。</p> <p>3.Recommender：基于对比学习优化的Transformer模型，智能推荐区分疾病所需的关键症状，提高诊断精度与置信度。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/phenodp/3-480.webp 480w,/assets/img/blog/phenodp/3-800.webp 800w,/assets/img/blog/phenodp/3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/phenodp/3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="图2 Summarizer的知识蒸馏示意图" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>性能评估与对比分析</p> <p>作者在四类模拟数据和三套真实数据集（共计5996个真实病例）上系统评估了PhenoDP的性能：</p> <p>1.模拟数据集：在所有难度类型下，PhenoDP的Top20覆盖率和平均倒数排名（MRR）均为第一，分别比次优方法提高11.7%和12.6%。</p> <p>2.真实数据集：在三组独立真实患者数据中，PhenoDP的Top1准确率分别高出8.1%、8.6%、2.6%。</p> <p>3.症状推荐能力：在目标疾病原本排序为第2/3的病例中，使用PhenoDP推荐的新症状后，有78.1%的病例目标疾病升至第一位，远优于GPT-4o（53.4%）和PhenoTips（23.3%）。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/phenodp/4-480.webp 480w,/assets/img/blog/phenodp/4-800.webp 800w,/assets/img/blog/phenodp/4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/phenodp/4.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="图3在真实数据集上对Ranker的评估结果" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/phenodp/5-480.webp 480w,/assets/img/blog/phenodp/5-800.webp 800w,/assets/img/blog/phenodp/5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/phenodp/5.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="图4对Recommender的评估结果" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>案例分析：免疫缺陷病IMD103</p> <p>研究以IMD103为例，初始排序中该疾病与其他免疫缺陷病（如IMD25）分数接近。PhenoDP的Recommender成功推荐出区分性症状“淋巴结病”，使IMD103得分显著上升并拉开与相似疾病的差距；而GPT-4o和PhenoTips推荐的症状缺乏区分度，甚至导致排序错误。</p> <p>实用性与未来应用</p> <p>PhenoDP已开源发布（https://github.com/TianLab-Bioinfo/PhenoDP），支持一键生成结构化报告，输出临床摘要、候选疾病列表与症状推荐结果，便于医生快速决策与后续诊断。模块化设计还便于未来集成基因信息、电子病历或进一步优化语义建模。</p> <p>综上，PhenoDP通过深度学习技术革新了表型驱动的诊断流程，为罕见病诊断提供了高效、精准的解决方案。该工作有望加速临床决策，改善患者预后。</p> <p>复旦大学生命科学学院计算生物学系直博生温宝乐为本文第一作者,田卫东教授为通讯作者。该研究得到了国家自然科学基金的支持。</p> <p>原文链接：https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-025-01496-8#Sec37</p>]]></content><author><name></name></author></entry><entry><title type="html">Association of human-specific expanded short tandem repeats with neuron-specific regulatory features</title><link href="https://tianlab-bioinfo.github.io/blog/2025/str/" rel="alternate" type="text/html" title="Association of human-specific expanded short tandem repeats with neuron-specific regulatory features"/><published>2025-06-06T14:24:00+00:00</published><updated>2025-06-06T14:24:00+00:00</updated><id>https://tianlab-bioinfo.github.io/blog/2025/str</id><content type="html" xml:base="https://tianlab-bioinfo.github.io/blog/2025/str/"><![CDATA[<p>2025年5月30日，复旦大学生命科学学院田卫东教授团队在Science Advances发表题为Association of human-specific expanded short tandem repeats with neuron-specific regulatory features的研究论文。该研究首次系统性地鉴定了8813个人类特异性扩增的短串联重复序列（heSTRs），并揭示了它们在神经元特异性调控中的重要作用，为理解人类大脑进化机制和神经发育疾病的遗传基础提供了全新视角。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/str/1-480.webp 480w,/assets/img/blog/str/1-800.webp 800w,/assets/img/blog/str/1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/str/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>背景</p> <p>在人类进化的漫长历程中，人类与非人灵长类动物逐渐分化，发展出了独特的认知能力和神经解剖特征。尽管人类与非人灵长类动物的氨基酸序列差异微小，但在大脑功能和认知能力方面却存在显著差异。这种表型分化主要由调控突变驱动，早期研究主要关注高度保守的非编码调控区域，如人类加速进化区域（HARs）等，这些区域与神经功能密切相关。然而，短串联重复序列（STRs）作为基因组中变异最快的元件之一，其拷贝数突变率比单核苷酸变异高出几个数量级，在人类特异性表型进化中的作用机制一直缺乏系统性研究。</p> <p>全基因组识别heSTRs的流程设计</p> <p>为了可靠地鉴定人类特异性扩增的STRs，研究团队设计了严格的筛选策略。从hg38参考基因组的670,429个STRs出发，通过多步骤统计检验：首先筛选出160,054个在人类和非人灵长类中都存在的同源STRs，再通过单峰性检验和分布一致性检验获得88,267个保守型ncSTRs，最终通过Wilcoxon检验比较148个人类基因组与26个非人灵长类基因组的拷贝数差异，鉴定出8813个在人类中显著扩增的heSTRs。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/str/2-480.webp 480w,/assets/img/blog/str/2-800.webp 800w,/assets/img/blog/str/2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/str/2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="图1 人类特异性短串联重复序列的全基因组鉴定流程" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>核心发现一：heSTRs与神经元特异性调控特征密切关联</p> <p>研究发现，heSTRs显著富集于大脑和神经元特异性的调控信号中。与保守型STRs相比，heSTRs与脑特异性DHS（DNase超敏位点）的关联性更强，并且富集在大脑特异的增强子中，提示其在远程调控中发挥重要作用。通过整合3D基因组结构数据，研究进一步发现heSTRs显著富集于神经元特异性的染色质环结构和拓扑关联结构域中。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/str/3-480.webp 480w,/assets/img/blog/str/3-800.webp 800w,/assets/img/blog/str/3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/str/3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="图2 heSTRs多种调控元件注释的关联分析" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>核心发现二：潜在靶基因富集于神经元发育功能</p> <p>通过整合远程调控信息，研究鉴定了6542个可能受heSTRs调控的潜在靶基因。这些基因显著富集于神经元发育相关的生物学过程，包括”神经元突起形态发生”、”突触组织”和”神经元突起发育调控”等功能。跨物种表达分析验证了heSTRs的调控效应：受heSTRs调控的基因在人类大脑中表现出特异性的表达增强，特别是在神经元细胞中。</p> <p>核心发现三：与神经发育疾病的重要关联</p> <p>研究还发现heSTRs与多种人群层面功能性STRs存在显著关联，并且在人群中受到更强的选择约束，基因组约束评分显著高于保守型STRs，表明其功能重要性。更重要的是，heSTRs与已知的致病性STRs存在显著关联。通过与精神分裂症和自闭症队列研究中鉴定的疾病相关罕见扩增串联重复进行比较分析，发现heSTRs显著富集于这些疾病相关位点中。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/str/4-480.webp 480w,/assets/img/blog/str/4-800.webp 800w,/assets/img/blog/str/4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/str/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="图3 heSTRs与致病性STRs的关联分析" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>研究意义</p> <p>该研究首次系统性地揭示了人类特异性扩增的短串联重复序列与神经元特异性调控的密切关联，强调了heSTRs作为此前被忽视的神经元特异性调控元件的重要性。这些发现不仅为理解人类大脑进化的分子机制提供了新视角，也为神经发育疾病的遗传基础研究提供了重要线索，heSTRs资源有望帮助优先筛选和解释临床队列中发现的致病性STR变异。</p> <p>复旦大学生命科学学院博士生刘启明为本文第一作者，田卫东教授为通讯作者。该研究得到了国家自然科学基金和国家重点研发计划等项目资助。</p> <p>论文链接： https://www.science.org/doi/10.1126/sciadv.adp9707</p>]]></content><author><name></name></author><category term="news"/><category term="STR"/><summary type="html"><![CDATA[2025年5月30日，复旦大学生命科学学院田卫东教授团队在Science Advances发表题为Association of human-specific expanded short tandem repeats with neuron-specific regulatory features的研究论文。该研究首次系统性地鉴定了8813个人类特异性扩增的短串联重复序列（heSTRs），并揭示了它们在神经元特异性调控中的重要作用，为理解人类大脑进化机制和神经发育疾病的遗传基础提供了全新视角。]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://tianlab-bioinfo.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://tianlab-bioinfo.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://tianlab-bioinfo.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024[[read-time]] min read We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://tianlab-bioinfo.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://tianlab-bioinfo.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://tianlab-bioinfo.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>